# ABAE: Utilize Attention to Boost Graph Auto-Encoder

This is the code for the paper "**ABAE: Utilize Attention to Boost Graph Auto-Encoder, PRICAI 2021**". Our architecture is based on the encoder-decoder model. The components of this model are shown as follows.

![](https://i.loli.net/2021/06/10/exbZRm5M1iP2Fk8.png)

### Dependencies

- Python 3.7+
- Pytorch 1.7.0+
- Numpy 1.17.2+
- tqdm 4.41.1+



### Results

Our architecture has achieved state-of-the-art in *Link Prediction*. Total comparance is given.

![image-20210610103103064](https://i.loli.net/2021/06/10/6VUGZFHD2Jbxhmo.png)

### Citation
Cite this paper as:
Liu T., Li Y., Sun Y., Cui L., Bai L. (2021) ABAE: Utilize Attention to Boost Graph Auto-Encoder. In: Pham D.N., Theeramunkong T., Governatori G., Liu F. (eds) PRICAI 2021: Trends in Artificial Intelligence. PRICAI 2021. Lecture Notes in Computer Science, vol 13032. Springer, Cham. https://doi.org/10.1007/978-3-030-89363-7_26
